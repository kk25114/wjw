# Sqoop 数据迁移指南
该文件为硬编码，临时快速执行，使用自动化脚本参考 sqoop/1.import_mysql8.sh，2.import_oracle_by_year.sh,
在脚本修改参数指定表名
## MySQL 数据导入 Hive

### 从 MySQL 导入数据到 Hive
```bash
✅ 成功
# 连接MySQL数据库,从wjw_wk库导入数据到Hive
# --connect: 指定MySQL数据库连接URL
# --username/--password: MySQL数据库用户名和密码
# --hive-import: 导入到Hive
# --hive-overwrite: 覆盖已有的Hive表
# --hive-database: 指定Hive数据库名
# --hive-table: 指定Hive表名
# --target-dir: 指定HDFS临时目录
# --create-hive-table: 如果表不存在则创建
# --delete-target-dir: 如果目标目录存在则删除
# --num-mappers: 指定map任务数量
# --null-string/--null-non-string: 指定NULL值的字符串表示
# --hive-drop-import-delims: 去除分隔符
# --fields-terminated-by: 指定字段分隔符
# --compression-codec: 指定压缩格式
# --query: 指定在mysql中的查询语句
sqoop import \
--connect jdbc:mysql://192.168.50.171:13306/wjw_wk \
--username root \
--password root \
--hive-import \
--hive-overwrite \
--hive-database hhrdc_signup_manage \
--hive-table dw_kw_bmk_t \
--target-dir /tmp/dw_kw_bmk_t \
--create-hive-table \
--delete-target-dir \
--num-mappers 1 \
--null-string '\\N' \
--null-non-string '\\N' \
--hive-drop-import-delims \
--fields-terminated-by '\t' \
--compression-codec snappy \
--query 'SELECT * FROM dw_kw_bmk_t WHERE $CONDITIONS'

impala-shell -q "refresh wjw_wk.dw_kw_bmk_t;"
impala-shell -q "select * from wjw_wk.dw_kw_bmk_t;"

impala-shell -q "drop database if exists hhrdc_signup_manage cascade; create database hhrdc_signup_manage;"


```

## Oracle 数据导入 Hive

### 注意事项
- `--create-hive-table` 参数冲突（因为表已存在会报错）
- `--as-parquetfile` 参数冲突（要建表再parquet）

### 从 Oracle 导入数据到 Hive
```bash
✅ 成功
方法一：
sqoop import \
--connect jdbc:oracle:thin:@192.168.50.171:1521:XE \
--username exam2022 \
--password exam2022 \
--hive-import \
--hive-overwrite \
--hive-database hhrdc_signup_manage \
--hive-table dw_kw_bmk_t \
--target-dir /root/dw_kw_bmk_t \
--delete-target-dir \
--num-mappers 1 \
--null-string '\\N' \
--null-non-string '\\N' \
--hive-drop-import-delims \
--fields-terminated-by '\t' \
--create-hive-table \
--compression-codec snappy \
--query 'SELECT * FROM DW_KW_BMK_T WHERE $CONDITIONS'

```
方法二：
```bash
✅ 成功
sqoop import \
--connect jdbc:oracle:thin:@192.168.50.171:1521:XE \
--username exam2022 \
--password exam2022 \
--table DW_KW_BMK_T  \
--hive-import \
--hive-overwrite \
--hive-database hhrdc_signup_manage \
--hive-table dw_kw_bmk_t \
--target-dir /root/dw_kw_bmk_t \
--delete-target-dir \
--num-mappers 1 \
--null-string '\\N' \
--null-non-string '\\N' \
--hive-drop-import-delims \
--fields-terminated-by '\t' \
--create-hive-table
```











## 数据导出操作

### 导出到 MySQL
> 注意：需要在 MySQL 中提前建表

```bash
hive -e "INSERT OVERWRITE DIRECTORY '/tmp/export_data' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' \
SELECT * FROM wjw_wk.dw_kw_bmk_t WHERE ymonth='2024-05';" ; \
sqoop export \
--connect jdbc:mysql://192.168.50.171:13306/wjw_wk \
--username root \
--password root \
--table dw_kw_bmk_t \
--export-dir /tmp/export_data \
--input-fields-terminated-by '\001' \
--input-lines-terminated-by '\n' \
--input-null-string '\\N' \
--input-null-non-string '\\N' \
--num-mappers 1 \
--columns "ksid,name,dah,xb_old,mz_old,xl_old,xw_old,byzy_old,sr,xzqy_old,ssdm_old,sqdm_old,qxdm_old,dwmc_old,dwss_old,dwxz_old,gznx,ksh,zjlx_old,zjh,xyjszg_old,kslx_old,jgjb_old,xb,mz,xl,xw,byzy,xzqy,ssdm,sqdm,qxdm,dwmc,dwss,dwxz,zjlx,xyjszg,kslx,jgjb,xzqy_gb,ssdm_gb,sqdm_gb,qxdm_gb"
```

### 导出到 Oracle
```bash
sqoop export \
--connect jdbc:oracle:thin:@192.168.50.171:1521:XE \
--username exam2022 \
--password exam2022 \
....
```